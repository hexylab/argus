# Issue #56: GPU 環境構築 実装計画

**作成日時**: 2026-01-03 23:12
**Issue**: #56 GPU 環境構築
**ブランチ**: `feature/#56-gpu-environment`

## 概要

SigLIP 2 / SAM 3 の推論を実行するための GPU Worker 環境を構築する。

## 完了条件

- [ ] GPU Worker 用 Dockerfile 作成
- [ ] docker-compose への GPU サービス追加
- [ ] ローカル GPU 対応 (CUDA 12.1+)
- [ ] モデルキャッシュ用ボリューム設定
- [ ] 動作確認スクリプト作成

## 設計

### アーキテクチャ

```
┌─────────────────────────────────────────────────────────────────┐
│ Docker Environment                                               │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────────────┐  │
│  │   Backend    │  │ Celery Worker│  │   GPU Worker         │  │
│  │  (FastAPI)   │  │  (CPU tasks) │  │  (ML inference)      │  │
│  │              │  │              │  │                      │  │
│  │  API         │  │  Frame       │  │  SigLIP 2            │  │
│  │  Endpoints   │  │  Extraction  │  │  SAM 3               │  │
│  │              │  │  (FFmpeg)    │  │                      │  │
│  └──────┬───────┘  └──────┬───────┘  └──────────┬───────────┘  │
│         │                 │                      │              │
│         └─────────────────┼──────────────────────┘              │
│                           │                                     │
│                    ┌──────▼──────┐                              │
│                    │    Redis    │                              │
│                    │  (Celery)   │                              │
│                    └─────────────┘                              │
└─────────────────────────────────────────────────────────────────┘
```

### 方針

1. **既存 Worker と分離**
   - 既存の `celery-worker` は CPU タスク（FFmpeg フレーム抽出）専用
   - 新規 `gpu-worker` は GPU タスク（SigLIP 2, SAM 3）専用
   - Celery のキューで分離 (`default` vs `gpu`)

2. **ベースイメージ**
   - `nvidia/cuda:12.1.0-cudnn8-runtime-ubuntu22.04`（runtime で軽量化）
   - Python 3.12 + uv（既存との一貫性）

3. **モデルキャッシュ**
   - HuggingFace モデルを永続化ボリュームにキャッシュ
   - 初回起動時のダウンロード時間を削減

## 実装ファイル

### 新規作成

| ファイル | 説明 |
|----------|------|
| `backend/Dockerfile.gpu` | GPU Worker 用 Dockerfile |
| `docker/docker-compose.gpu.yml` | GPU サービス定義 |
| `backend/scripts/verify_gpu.py` | GPU 動作確認スクリプト |

### 修正

| ファイル | 説明 |
|----------|------|
| `Makefile` | GPU 関連コマンド追加 |

## 実装詳細

### 1. backend/Dockerfile.gpu

```dockerfile
# Argus GPU Worker Dockerfile
# For SigLIP 2 / SAM 3 inference

FROM nvidia/cuda:12.1.0-cudnn8-runtime-ubuntu22.04

# Non-interactive installation
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1

# Timezone
RUN ln -snf /usr/share/zoneinfo/UTC /etc/localtime && echo UTC > /etc/timezone

# System packages
RUN apt-get update && apt-get install -y --no-install-recommends \
    software-properties-common \
    curl \
    git \
    # OpenCV dependencies
    libgl1-mesa-glx \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    libgomp1 \
    && rm -rf /var/lib/apt/lists/*

# Python 3.12
RUN add-apt-repository ppa:deadsnakes/ppa -y && \
    apt-get update && \
    apt-get install -y --no-install-recommends \
    python3.12 \
    python3.12-dev \
    python3.12-venv \
    && rm -rf /var/lib/apt/lists/* && \
    update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.12 1

# Install uv
COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv

# Working directory
WORKDIR /app

# HuggingFace cache
ENV HF_HOME="/models/.cache"
ENV TRANSFORMERS_CACHE="/models/.cache/transformers"

# Copy dependency files
COPY pyproject.toml uv.lock README.md ./

# Install dependencies (GPU extras will be added later)
RUN uv sync --frozen --no-dev

# Copy application code
COPY . .

# Default: run Celery worker on GPU queue
CMD ["uv", "run", "celery", "-A", "app.celery", "worker", "--loglevel=info", "-Q", "gpu"]
```

### 2. docker/docker-compose.gpu.yml

```yaml
# GPU Worker services
# Usage: docker compose -f docker-compose.yml -f docker-compose.dev.yml -f docker-compose.gpu.yml up

services:
  gpu-worker:
    build:
      context: ../backend
      dockerfile: Dockerfile.gpu
    container_name: argus-gpu-worker
    environment:
      # Supabase
      - SUPABASE_URL=http://host.docker.internal:54331
      - SUPABASE_ANON_KEY=${SUPABASE_ANON_KEY}
      - SUPABASE_JWT_SECRET=${SUPABASE_JWT_SECRET}
      # Database
      - DATABASE_URL=postgresql://postgres:postgres@host.docker.internal:54332/postgres
      # Redis & MinIO
      - REDIS_URL=redis://redis:6379/0
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=${MINIO_ROOT_USER:-minioadmin}
      - MINIO_SECRET_KEY=${MINIO_ROOT_PASSWORD:-minioadmin}
      # Model cache
      - HF_HOME=/models/.cache
      - TRANSFORMERS_CACHE=/models/.cache/transformers
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ../backend:/app
      - model_cache:/models
    depends_on:
      redis:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: uv run celery -A app.celery worker --loglevel=info -Q gpu

volumes:
  model_cache:
```

### 3. backend/scripts/verify_gpu.py

```python
#!/usr/bin/env python3
"""GPU 環境動作確認スクリプト"""

import sys


def check_cuda():
    """CUDA の確認"""
    try:
        import torch

        print(f"PyTorch version: {torch.__version__}")
        print(f"CUDA available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"CUDA version: {torch.version.cuda}")
            print(f"GPU count: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
            return True
        return False
    except ImportError:
        print("PyTorch not installed")
        return False


def check_transformers():
    """Transformers の確認"""
    try:
        import transformers

        print(f"Transformers version: {transformers.__version__}")
        return True
    except ImportError:
        print("Transformers not installed")
        return False


def main():
    print("=" * 50)
    print("GPU Environment Verification")
    print("=" * 50)

    results = []

    print("\n[1/2] Checking CUDA...")
    results.append(("CUDA", check_cuda()))

    print("\n[2/2] Checking Transformers...")
    results.append(("Transformers", check_transformers()))

    print("\n" + "=" * 50)
    print("Summary")
    print("=" * 50)
    all_passed = True
    for name, passed in results:
        status = "OK" if passed else "FAIL"
        print(f"  {name}: {status}")
        if not passed:
            all_passed = False

    if all_passed:
        print("\nAll checks passed!")
        return 0
    else:
        print("\nSome checks failed.")
        return 1


if __name__ == "__main__":
    sys.exit(main())
```

### 4. Makefile 追加

```makefile
# GPU Worker commands
up-gpu:
	docker compose -f docker/docker-compose.yml -f docker/docker-compose.dev.yml -f docker/docker-compose.gpu.yml up -d

down-gpu:
	docker compose -f docker/docker-compose.yml -f docker/docker-compose.dev.yml -f docker/docker-compose.gpu.yml down

verify-gpu:
	docker compose -f docker/docker-compose.yml -f docker/docker-compose.dev.yml -f docker/docker-compose.gpu.yml \
		run --rm gpu-worker uv run python scripts/verify_gpu.py
```

## 実装順序

1. `backend/Dockerfile.gpu` を作成
2. `docker/docker-compose.gpu.yml` を作成
3. `backend/scripts/verify_gpu.py` を作成
4. `Makefile` に GPU コマンドを追加
5. 動作確認

## 注意事項

- GPU がない環境では `docker-compose.gpu.yml` を使用しない
- NVIDIA Container Toolkit が必要: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html
- 初回起動時は PyTorch のダウンロードに時間がかかる

## 参照

- r2s2 プロジェクト: `docker/cloud/Dockerfile.sam3`
- NVIDIA Container Toolkit: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/
